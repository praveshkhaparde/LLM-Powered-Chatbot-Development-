{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce7a83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32197101",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello Welcome, to Ami Gogia's NLP Tutorials.\n",
    "Ami Gogia is gonna build a chatbot or he won't get a SOC certificate! XD\"\"\"\n",
    "## Corpus = Paragraph\n",
    "## Triple Quotes for multi-line string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b3276e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "temp_str = \"\"\n",
    "for ch in corpus:\n",
    "    if ch in ['.', '!', '?']:\n",
    "        documents.append(temp_str)\n",
    "        temp_str = \"\"\n",
    "    else:\n",
    "        temp_str += ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b76ed096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome, to Ami Gogia's NLP Tutorials\",\n",
       " \"\\nAmi Gogia is gonna build a chatbot or he won't get a SOC certificate\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3276ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "temp_str = \"\"\n",
    "for ch in corpus:\n",
    "    if ch in ['.', '!', '?']:\n",
    "        temp_str += ch\n",
    "        documents.append(temp_str)\n",
    "        temp_str = \"\"\n",
    "    elif ch == '\\n':\n",
    "        pass\n",
    "    else:\n",
    "        temp_str += ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45ab7cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome, to Ami Gogia's NLP Tutorials.\",\n",
       " \"Ami Gogia is gonna build a chatbot or he won't get a SOC certificate!\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23f19d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sent_tokenize(corpus):\n",
    "    documents = []\n",
    "    temp_str = \"\"\n",
    "    for ch in corpus:\n",
    "        if ch in ['.', '!', '?']:\n",
    "            temp_str += ch\n",
    "            documents.append(temp_str)\n",
    "            temp_str = \"\"\n",
    "        elif ch == '\\n':\n",
    "            pass\n",
    "        elif ch == ' ' and temp_str == '':\n",
    "            pass\n",
    "        else:\n",
    "            temp_str += ch\n",
    "    if temp_str:\n",
    "        documents.append(temp_str)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c332402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2bfff833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sent_tokenize in module nltk.tokenize:\n",
      "\n",
      "sent_tokenize(text, language='english')\n",
      "    Return a sentence-tokenized copy of *text*,\n",
      "    using NLTK's recommended sentence tokenizer\n",
      "    (currently :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "\n",
      "    :param text: text to split into sentences\n",
      "    :param language: the model name in the Punkt corpus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb4fe973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome, to Ami Gogia's NLP Tutorials.\",\n",
       " \"Ami Gogia is gonna build a chatbot or he won't get a SOC certificate!\",\n",
       " 'XD']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sent_tokenize converts para to sentences\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4728ee39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome, to Ami Gogia's NLP Tutorials.\",\n",
       " \"Ami Gogia is gonna build a chatbot or he won't get a SOC certificate!\",\n",
       " 'XD']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fe7d5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi?', 'Everything okay?']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize('Hi? Everything okay?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab6ab058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "## word_tokenize does Sentence --> Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96860458",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d4a4792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " 'is',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'build',\n",
       " 'a',\n",
       " 'chatbot',\n",
       " 'or',\n",
       " 'he',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'get',\n",
       " 'a',\n",
       " 'SOC',\n",
       " 'certificate',\n",
       " '!',\n",
       " 'XD']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e00d32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Ami', 'Gogia', \"'s\", 'NLP', 'Tutorials', '.']\n",
      "['Ami', 'Gogia', 'is', 'gon', 'na', 'build', 'a', 'chatbot', 'or', 'he', 'wo', \"n't\", 'get', 'a', 'SOC', 'certificate', '!']\n",
      "['XD']\n"
     ]
    }
   ],
   "source": [
    "for sent in documents:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "849c1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "## \"'s\" also gets separated\n",
    "## Ensures that punctuation is also treated as a separate word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4929fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " 'is',\n",
       " 'gonna',\n",
       " 'build',\n",
       " 'a',\n",
       " 'chatbot',\n",
       " 'or',\n",
       " 'he',\n",
       " 'won',\n",
       " \"'\",\n",
       " 't',\n",
       " 'get',\n",
       " 'a',\n",
       " 'SOC',\n",
       " 'certificate',\n",
       " '!',\n",
       " 'XD']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8027d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TreeBank Tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d11722",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "## Full stop not considered as a separate word/token\n",
    "## except for the last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "438c4a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " 'is',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'build',\n",
       " 'a',\n",
       " 'chatbot',\n",
       " 'or',\n",
       " 'he',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'get',\n",
       " 'a',\n",
       " 'SOC',\n",
       " 'certificate',\n",
       " '!',\n",
       " 'XD']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92ad78c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Ami',\n",
       " 'Gogia',\n",
       " 'is',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'build',\n",
       " 'a',\n",
       " 'chatbot',\n",
       " 'or',\n",
       " 'he',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'get',\n",
       " 'a',\n",
       " 'SOC',\n",
       " 'certificate',\n",
       " '!',\n",
       " 'XD',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus += '.'\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different Tokenizers have different word-splitting rules\n",
    "## See which one fits your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
